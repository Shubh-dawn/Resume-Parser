{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b769eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\subha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\subha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from spacy.matcher import Matcher\n",
    "import io\n",
    "import docx2txt\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd2c873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SHUBHANSHU GUPTA +917906754549| subhanshugupta1995@gmail.com| LinkedIn| Github EDUCATION Graphic Era Deemed University        Dehradun, India  Bachelor of Technology in Computer Science, Secured an aggregate of 9.2 CGPA                July 2020 – 2024    PROJECTS Movie Recommendation Web Page | Python, NLP, Streamlit, API • This is a content-based filtering movie recommendation system, which suggests the user movie related to the movie searched for. • Used TDMB API to fetch the movie posters of the recommended movies to make it more efficient. Image Caption Generator| Python, LSTM, Flask, ResNet50 • Created a web page using flask which generates a caption for the given image.  • Used the flickr8k_sau dataset and ResNet5 model to extract the feature vectors from images and then further applied LSTM to predict the caption for the image. WikiImage | Python, Flask, Deep Learning • Developed a project that recognizes any famous person’s face with the help of Computer Vision, Image Processing and then showing their brief introduction, tweets, songs, and movies. • Used Face Recognition for getting face encodings, Open-CV for processing the image, and Pickle for dumping the encodings into the binary file. Video Analysis Using Computer Vision | Python, ML • Designed a Face-based frame extraction from a long video to calculate the total time a person is available in an event using face detection and face recognition algorithms. • Used Face Recognition to get face encodings, Open-CV for processing the image, and Tkinter for creating GUI. SKILLS C, C++, Java, Python, SQL, HTML, CSS, and Javascript.  COURSE WORK Data Structures, Object Oriented Programming, Algorithm Design and Analysis, Operating Systems, Database Management Systems, and Web Services.  FRAMEWORKS AND TOOLS  Machine Learning, Deep Learning, OpenCV, Flask, Web Scraping, Scikit-Learn, NLP, Shell Scripting, Linux.   ACHIEVEMENTS Earned Google Cloud Badges: https://www.cloudskillsboost.google/public_profiles/fd1b0426-398a-49bb-a9b9-62bc1f1a6b00. Hacktoberfest 2022: Participated in Hacktoberfest and received holopin badges and the badge of completion for open-source contribution. Goldman Sachs Engineering Virtual Internship: I Completed The virtual internship program of Goldman Sachs through forage on cracking leaked password databases. I have also learned about the different types of hashing algorithms and how we should frame our passwords for maximum security in the case of a data breach.  \f",
      "\n"
     ]
    }
   ],
   "source": [
    "#method to extract text from pdf\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        # iterate over all pages of PDF document\n",
    "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "            # creating a resoure manager\n",
    "            resource_manager = PDFResourceManager()\n",
    "            \n",
    "            # create a file handle\n",
    "            fake_file_handle = io.StringIO()\n",
    "            \n",
    "            # creating a text converter object\n",
    "            converter = TextConverter(resource_manager, \n",
    "                                    fake_file_handle) \n",
    "\n",
    "            # creating a page interpreter\n",
    "            page_interpreter = PDFPageInterpreter(\n",
    "                                resource_manager, \n",
    "                                converter\n",
    "                            )\n",
    "\n",
    "            # process current page\n",
    "            page_interpreter.process_page(page)\n",
    "            \n",
    "            # extract text\n",
    "            text = fake_file_handle.getvalue()\n",
    "            yield text\n",
    "\n",
    "            # close open handles\n",
    "            converter.close()\n",
    "            fake_file_handle.close()\n",
    "\n",
    "# calling above function and extracting text\n",
    "text=\"\"\n",
    "for page in extract_text_from_pdf('S:/Documents/Latest.pdf'):\n",
    "    text += ' ' + page\n",
    "    \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3627795f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHUBHANSHU GUPTA +917906754549| subhanshugupta1995@gmail.com| LinkedIn| Github EDUCATION Graphic Era Deemed University        Dehradun, India  Bachelor of Technology in Computer Science, Secured an aggregate of 9.2 CGPA                July 2020 – 2024    PROJECTS Movie Recommendation Web Page | Python, NLP, Streamlit, API This is a content-based filtering movie recommendation system, which suggests the user movie related to the movie searched for. Used TDMB API to fetch the movie posters of the recommended movies to make it more efficient. Image Caption Generator| Python, LSTM, Flask, ResNet50 Created a web page using flask which generates a caption for the given image.  Used the flickr8k_sau dataset and ResNet5 model to extract the feature vectors from images and then further applied LSTM to predict the caption for the image. WikiImage | Python, Flask, Deep Learning Developed a project that recognizes any famous person’s face with the help of Computer Vision, Image Processing and then showing their brief introduction, tweets, songs, and movies. Used Face Recognition for getting face encodings, Open-CV for processing the image, and Pickle for dumping the encodings into the binary file. Video Analysis Using Computer Vision | Python, ML Designed a Face-based frame extraction from a long video to calculate the total time a person is available in an event using face detection and face recognition algorithms. Used Face Recognition to get face encodings, Open-CV for processing the image, and Tkinter for creating GUI. SKILLS C, C++, Java, Python, SQL, HTML, CSS, and Javascript. COURSE WORK Data Structures, Object Oriented Programming, Algorithm Design and Analysis, Operating Systems, Database Management Systems, and Web Services. FRAMEWORKS AND TOOLS  Machine Learning, Deep Learning, OpenCV, Flask, Web Scraping, Scikit-Learn, NLP, Shell Scripting, Linux.  ACHIEVEMENTS Earned Google Cloud Badges: https://www.cloudskillsboost.google/public_profiles/fd1b0426-398a-49bb-a9b9-62bc1f1a6b00. Hacktoberfest 2022: Participated in Hacktoberfest and received holopin badges and the badge of completion for open-source contribution. Goldman Sachs Engineering Virtual Internship: I Completed The virtual internship program of Goldman Sachs through forage on cracking leaked password databases. I have also learned about the different types of hashing algorithms and how we should frame our passwords for maximum security in the case of a data breach.\n"
     ]
    }
   ],
   "source": [
    "#method to extract text from a docx\n",
    "\n",
    "def extract_text_from_doc(doc_path):\n",
    "    temp = docx2txt.process(doc_path)\n",
    "    text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
    "    return ' '.join(text)\n",
    "\n",
    "print(extract_text_from_doc(\"S:/Documents/cv.docx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c2c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec2f77e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subhanshugupta1995@gmail.com|\n"
     ]
    }
   ],
   "source": [
    "#Extracting the Email\n",
    "\n",
    "def extract_email(email):\n",
    "\n",
    "    pattern =[{\"LIKE_EMAIL\":True}]\n",
    "    matcher.add(\"EMAIL\",[pattern])\n",
    "\n",
    "    # text = \"You can contact Data Science Learner through email address contact@datasciencelearner.com\"\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    for match_id,start,end in matches:\n",
    "        return doc[start:end]\n",
    "        \n",
    "print(extract_email(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d3c4ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHUBHANSHU\n"
     ]
    }
   ],
   "source": [
    "#Extracting the name\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', [pattern])\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "\n",
    "print(extract_name(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b980864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9179067545\n"
     ]
    }
   ],
   "source": [
    "#Extracting Phone Number\n",
    "import re\n",
    "def extract_mobile_number(text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)    \n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number\n",
    "        \n",
    "print(extract_mobile_number(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46305d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyresparser import ResumeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94ec7af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:895: UserWarning: [W094] Model 'en_training' (0.0.0) specifies an under-constrained spaCy version requirement: >=2.1.4. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.5.1,<3.6.0\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read config file from C:\\Users\\subha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyresparser\\config.cfg",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m resume_data \u001b[38;5;241m=\u001b[39m \u001b[43mResumeParser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mS:/Documents/Latest.pdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_extracted_data()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyresparser\\resume_parser.py:21\u001b[0m, in \u001b[0;36mResumeParser.__init__\u001b[1;34m(self, resume, skills_file, custom_regex)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     16\u001b[0m     resume,\n\u001b[0;32m     17\u001b[0m     skills_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     18\u001b[0m     custom_regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     19\u001b[0m ):\n\u001b[0;32m     20\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m     custom_nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__skills_file \u001b[38;5;241m=\u001b[39m skills_file\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__custom_regex \u001b[38;5;241m=\u001b[39m custom_regex\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:444\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_package(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[1;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexists\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Path or Path-like to model data\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:515\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    513\u001b[0m config_path \u001b[38;5;241m=\u001b[39m model_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m overrides \u001b[38;5;241m=\u001b[39m dict_to_dot(config)\n\u001b[1;32m--> 515\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m nlp \u001b[38;5;241m=\u001b[39m load_model_from_config(\n\u001b[0;32m    517\u001b[0m     config,\n\u001b[0;32m    518\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[0;32m    523\u001b[0m )\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mfrom_disk(model_path, exclude\u001b[38;5;241m=\u001b[39mexclude, overrides\u001b[38;5;241m=\u001b[39moverrides)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:691\u001b[0m, in \u001b[0;36mload_config\u001b[1;34m(path, overrides, interpolate)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m--> 691\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE053\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mconfig_path, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig file\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\u001b[38;5;241m.\u001b[39mfrom_disk(\n\u001b[0;32m    693\u001b[0m         config_path, overrides\u001b[38;5;241m=\u001b[39moverrides, interpolate\u001b[38;5;241m=\u001b[39minterpolate\n\u001b[0;32m    694\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: [E053] Could not read config file from C:\\Users\\subha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyresparser\\config.cfg"
     ]
    }
   ],
   "source": [
    "resume_data = ResumeParser('S:/Documents/Latest.pdf').get_extracted_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2c9d81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
